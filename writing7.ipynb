{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dfs = [pd.read_csv(f'output/feat_df_{str(i)}').drop('Unnamed: 0', axis=1) for i in range(6)]\n",
    "combined_feats = pd.read_csv('output/combined_feats').drop('Unnamed: 0', axis=1)\n",
    "train_scores = pd.read_csv('input/train_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: validation set for pattern info: 0.82\n",
      "MSQE: validation set for n-gram tdidf: 0.703\n",
      "MSQE: validation set for iki info: 0.725\n",
      "MSQE: validation set for agg info: 0.657\n",
      "MSQE: validation set for other count info: 0.726\n",
      "MSQE: validation set for gap info: 0.685\n"
     ]
    }
   ],
   "source": [
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "df_names = ['pattern info', 'n-gram tdidf', 'iki info', 'agg info', 'other count info', 'gap info']\n",
    "y_trains = []\n",
    "y_valids = []\n",
    "for i, df in enumerate(feature_dfs):\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train[df.columns], y_train)\n",
    "    y_predict_train = model.predict(X_train[df.columns])\n",
    "    y_predict_valid = model.predict(X_valid[df.columns])\n",
    "    print(f'MSQE: validation set for {df_names[i]}: {round(mean_squared_error(y_valid, y_predict_valid, squared=False), 3)}')\n",
    "    y_trains.append(y_predict_train)\n",
    "    y_valids.append(y_predict_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set, 0.467\n",
      "MSQE: validation set for gap info, 0.626\n"
     ]
    }
   ],
   "source": [
    "weights = [.1, .1, .1, .35, .1, .25]\n",
    "y_train_combined, y_valids_combined = y_trains[0] * weights[0], y_valids[0] * weights[0]\n",
    "for i in range(1, len(weights)):\n",
    "    y_train_combined += y_trains[i] * weights[i]\n",
    "    y_valids_combined += y_valids[i] * weights[i]\n",
    "print(f'MSQE: training set, {round(mean_squared_error(y_train_combined, y_train, squared=False), 3)}')\n",
    "print(f'MSQE: validation set for {df_names[i]}, {round(mean_squared_error(y_valids_combined, y_valid, squared=False), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "import tensorflow as tf\n",
    "\n",
    "rs=0\n",
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "def print_results(model_name, gs):\n",
    "    print(f'Best Score for {model_name}: {round(-gs.best_score_, 3)}, Best Params: {gs.best_params_}')\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Ridge',\n",
    "        'model': Ridge,\n",
    "        'params': {\n",
    "            'alpha': [0, .1, 1, 10],\n",
    "            'fit_intercept'    : [True, False]\n",
    "        },\n",
    "        'preprocess': lambda X: X\n",
    "    },\n",
    "    {\n",
    "        'name': 'Ridge with Polynomial Features',\n",
    "        'model': Ridge,\n",
    "        'params': {\n",
    "            'alpha': [0, .1, 1, 10],\n",
    "            'fit_intercept': [True, False]\n",
    "        },\n",
    "        'preprocess': lambda X: PolynomialFeatures(degree=2).fit_transform(StandardScaler().fit_transform(X))\n",
    "    },\n",
    "        {\n",
    "        'name': 'Lasso',\n",
    "        'model': Lasso,\n",
    "        'params': {\n",
    "            'alpha': [0, .01, .1, .3],\n",
    "            'fit_intercept'    : [True, False]\n",
    "        },\n",
    "        'preprocess': lambda X: X\n",
    "    },\n",
    "    {\n",
    "        'name': 'Lasso with Polynomial Features',\n",
    "        'model': Lasso,\n",
    "        'params': {\n",
    "            'alpha': [0, .01, .1, .3],\n",
    "            'fit_intercept': [True, False]\n",
    "        },\n",
    "        'preprocess': lambda X: PolynomialFeatures(degree=2).fit_transform(StandardScaler().fit_transform(X))\n",
    "    },\n",
    "    {\n",
    "        'name': 'KNN',\n",
    "        'model': KNeighborsRegressor,\n",
    "        'params': {\n",
    "            'n_neighbors' : [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "        },\n",
    "        'preprocess': lambda X: StandardScaler().fit_transform(X)\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVR',\n",
    "        'model': SVR,\n",
    "        'params': {\n",
    "            'kernel' : ['linear', 'poly', 'rbf'],\n",
    "            'C' : [.01, .1, 1],\n",
    "            'degree' : [1, 2],\n",
    "            'gamma' : [.01, .1, .5]\n",
    "        },\n",
    "        'preprocess': lambda X: StandardScaler().fit_transform(X)\n",
    "    },\n",
    "    {\n",
    "        'name': 'GBR',\n",
    "        'model': GradientBoostingRegressor,\n",
    "        'params': {\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'subsample': [.5, .65, .8, .9, 1],\n",
    "            'max_depth': [2, 3, 8]\n",
    "        },\n",
    "        'preprocess': lambda X: X\n",
    "    },\n",
    "    {\n",
    "        'name': 'LGBM',\n",
    "        'model': LGBMRegressor,\n",
    "        'params': {\n",
    "            'subsample': [.5, .65, .8, .9, 1],\n",
    "            'learning_rate': [0.1, 0.03, 0.003],\n",
    "            'max_depth': [2, 3, 8],\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'num_leaves': [131072]\n",
    "        },\n",
    "        'preprocess': lambda X: X.set_axis([str(i) for i in range(X.shape[1])], axis=1)\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGB',\n",
    "        'model': XGBRegressor,\n",
    "        'params': {\n",
    "            'subsample': [.5, .65, .8, .9, 1],\n",
    "            'learning_rate': [0.1, 0.03, 0.003],\n",
    "            'max_depth': [2, 3, 8],\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "        },\n",
    "        'preprocess': lambda X: X\n",
    "    },\n",
    "]\n",
    "\n",
    "def run_tests(cols):\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        gs = GridSearchCV(estimator=model['model'](), param_grid=model['params'], \n",
    "            scoring='neg_root_mean_squared_error', cv=4)\n",
    "        gs.fit(model['preprocess'](X_train[cols]), y_train)\n",
    "        print_results(model['name'], gs)\n",
    " \n",
    "        # print_results('Ridge Model', gs)\n",
    "        tuned_model = model['model'](**gs.best_params_)\n",
    "        tuned_model.fit(model['preprocess'](X_train[cols]), y_train)\n",
    "        y_pred = tuned_model.predict(model['preprocess'](X_valid[cols]))\n",
    "        results[model['name']] = {\n",
    "            'score': round(-gs.best_score_, 3), \n",
    "            'y_pred': y_pred,\n",
    "            'model': tuned_model\n",
    "        }\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Normalization(input_shape=[len(cols)]),\n",
    "        tf.keras.layers.Dense(300, activation='relu'),\n",
    "        tf.keras.layers.Dense(100, activation='relu'),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer='sgd', metrics=['RootMeanSquaredError'])\n",
    "    model.fit(X_train[cols], y_train, epochs=30, validation_data=(X_valid[cols], y_valid))\n",
    "    results['NN'] = {\n",
    "        'score': round(model.evaluate(X_valid[cols], y_valid)[1], 3),\n",
    "        'y_pred': model.predict(X_valid[cols]),\n",
    "        'model': model\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find ensemble model for pattern info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for Ridge: 0.933, Best Params: {'alpha': 10, 'fit_intercept': True}\n",
      "Best Score for Ridge with Polynomial Features: 1.026, Best Params: {'alpha': 10, 'fit_intercept': True}\n",
      "Best Score for Lasso: 0.929, Best Params: {'alpha': 0.01, 'fit_intercept': True}\n",
      "Best Score for Lasso with Polynomial Features: 0.934, Best Params: {'alpha': 0.01, 'fit_intercept': True}\n",
      "Best Score for KNN: 0.903, Best Params: {'n_neighbors': 50}\n",
      "Best Score for SVR: 0.897, Best Params: {'C': 1, 'degree': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best Score for GBR: 0.875, Best Params: {'max_depth': 2, 'n_estimators': 50, 'subsample': 0.65}\n",
      "Best Score for LGBM: 0.873, Best Params: {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'num_leaves': 131072, 'subsample': 0.5}\n",
      "Best Score for XGB: 0.874, Best Params: {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.65}\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 1s 9ms/step - loss: 2.6120 - root_mean_squared_error: 1.6162 - val_loss: 1.1708 - val_root_mean_squared_error: 1.0820\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.1473 - root_mean_squared_error: 1.0711 - val_loss: 0.8432 - val_root_mean_squared_error: 0.9183\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.9857 - root_mean_squared_error: 0.9928 - val_loss: 1.4170 - val_root_mean_squared_error: 1.1904\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.9158 - root_mean_squared_error: 0.9570 - val_loss: 0.7956 - val_root_mean_squared_error: 0.8920\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8534 - root_mean_squared_error: 0.9238 - val_loss: 1.2925 - val_root_mean_squared_error: 1.1369\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8533 - root_mean_squared_error: 0.9238 - val_loss: 0.7777 - val_root_mean_squared_error: 0.8819\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8113 - root_mean_squared_error: 0.9007 - val_loss: 0.7555 - val_root_mean_squared_error: 0.8692\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7772 - root_mean_squared_error: 0.8816 - val_loss: 0.8077 - val_root_mean_squared_error: 0.8987\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8119 - root_mean_squared_error: 0.9011 - val_loss: 0.8785 - val_root_mean_squared_error: 0.9373\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7685 - root_mean_squared_error: 0.8766 - val_loss: 0.7371 - val_root_mean_squared_error: 0.8585\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7903 - root_mean_squared_error: 0.8890 - val_loss: 0.8948 - val_root_mean_squared_error: 0.9460\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7796 - root_mean_squared_error: 0.8829 - val_loss: 0.7955 - val_root_mean_squared_error: 0.8919\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7641 - root_mean_squared_error: 0.8741 - val_loss: 0.9334 - val_root_mean_squared_error: 0.9661\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7532 - root_mean_squared_error: 0.8679 - val_loss: 0.7442 - val_root_mean_squared_error: 0.8627\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7471 - root_mean_squared_error: 0.8644 - val_loss: 0.8816 - val_root_mean_squared_error: 0.9389\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7508 - root_mean_squared_error: 0.8665 - val_loss: 0.7175 - val_root_mean_squared_error: 0.8470\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7480 - root_mean_squared_error: 0.8649 - val_loss: 0.7153 - val_root_mean_squared_error: 0.8458\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7363 - root_mean_squared_error: 0.8581 - val_loss: 0.7385 - val_root_mean_squared_error: 0.8594\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7243 - root_mean_squared_error: 0.8510 - val_loss: 0.7395 - val_root_mean_squared_error: 0.8599\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7296 - root_mean_squared_error: 0.8541 - val_loss: 0.7539 - val_root_mean_squared_error: 0.8683\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7265 - root_mean_squared_error: 0.8524 - val_loss: 0.7128 - val_root_mean_squared_error: 0.8443\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7235 - root_mean_squared_error: 0.8506 - val_loss: 0.7765 - val_root_mean_squared_error: 0.8812\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7262 - root_mean_squared_error: 0.8522 - val_loss: 0.7269 - val_root_mean_squared_error: 0.8526\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7193 - root_mean_squared_error: 0.8481 - val_loss: 0.7463 - val_root_mean_squared_error: 0.8639\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7201 - root_mean_squared_error: 0.8486 - val_loss: 0.7289 - val_root_mean_squared_error: 0.8538\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7134 - root_mean_squared_error: 0.8447 - val_loss: 0.7208 - val_root_mean_squared_error: 0.8490\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7023 - root_mean_squared_error: 0.8381 - val_loss: 0.7148 - val_root_mean_squared_error: 0.8455\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7063 - root_mean_squared_error: 0.8404 - val_loss: 0.7784 - val_root_mean_squared_error: 0.8823\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7016 - root_mean_squared_error: 0.8376 - val_loss: 0.7551 - val_root_mean_squared_error: 0.8690\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7012 - root_mean_squared_error: 0.8374 - val_loss: 0.8018 - val_root_mean_squared_error: 0.8954\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8018 - root_mean_squared_error: 0.8954\n",
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "results_pattern = run_tests(feature_dfs[0].columns)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step\n",
      "Validation score for model: Ridge -- 0.923\n",
      "Validation score for model: Ridge with Polynomial Features -- 1.367\n",
      "Validation score for model: Lasso -- 0.914\n",
      "Validation score for model: Lasso with Polynomial Features -- 0.806\n",
      "Validation score for model: KNN -- 0.766\n",
      "Validation score for model: SVR -- 0.749\n",
      "Validation score for model: GBR -- 0.743\n",
      "Validation score for model: LGBM -- 0.745\n",
      "Validation score for model: XGB -- 0.74\n",
      "Validation score for model: NN -- 0.894\n",
      "Weighted ensemble of models for validation set: 0.74\n"
     ]
    }
   ],
   "source": [
    "changable_rs = 0\n",
    "to_print = []\n",
    "y_valid_preds_patterns = []\n",
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=.2, random_state=changable_rs)\n",
    "for model in models + [{'name': 'NN', 'preprocess': lambda X: X}]:\n",
    "    tuned_model = results_pattern[model['name']]['model']\n",
    "    y_pred = tuned_model.predict(model['preprocess'](X_valid[feature_dfs[0].columns]))\n",
    "    y_valid_preds_patterns.append({\n",
    "        'model': model['name'], \n",
    "        'features': 'patterns', \n",
    "        'y_valid_pred': y_pred if model['name'] != 'NN' else y_pred[:,0], \n",
    "        'score': mean_squared_error(y_pred, y_valid)})\n",
    "\n",
    "weights = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "y_valid_pred_pattern = []\n",
    "for i in range(len(y_valid_preds_patterns)):\n",
    "    sc = round(mean_squared_error(y_valid, y_valid_preds_patterns[i]['y_valid_pred']), 3)\n",
    "    print(f'Validation score for model: {y_valid_preds_patterns[i][\"model\"]} -- {sc}')\n",
    "for i in range(len(y_valid_preds_patterns)):\n",
    "    vect = y_valid_preds_patterns[i]['y_valid_pred'] * weights[i]\n",
    "    if i == 0:\n",
    "        y_valid_pred_pattern = vect\n",
    "    else:\n",
    "        y_valid_pred_pattern = y_valid_pred_pattern + vect\n",
    "sc = round(mean_squared_error(y_valid, y_valid_pred_pattern), 3)\n",
    "print(f'Weighted ensemble of models for validation set: {sc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find ensemble info for n-gram tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for Ridge: 0.996, Best Params: {'alpha': 0, 'fit_intercept': True}\n",
      "Best Score for Ridge with Polynomial Features: 1.476, Best Params: {'alpha': 10, 'fit_intercept': True}\n",
      "Best Score for Lasso: 0.996, Best Params: {'alpha': 0, 'fit_intercept': True}\n",
      "Best Score for Lasso with Polynomial Features: 0.876, Best Params: {'alpha': 0.01, 'fit_intercept': False}\n",
      "Best Score for KNN: 1.095, Best Params: {'n_neighbors': 100}\n",
      "Best Score for SVR: 0.76, Best Params: {'C': 1, 'degree': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best Score for GBR: 0.711, Best Params: {'max_depth': 2, 'n_estimators': 100, 'subsample': 1}\n",
      "Best Score for LGBM: 0.71, Best Params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'num_leaves': 131072, 'subsample': 0.5}\n",
      "Best Score for XGB: 0.712, Best Params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 1s 9ms/step - loss: 2.9456 - root_mean_squared_error: 1.7163 - val_loss: 1.0228 - val_root_mean_squared_error: 1.0113\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0932 - root_mean_squared_error: 1.0455 - val_loss: 1.0233 - val_root_mean_squared_error: 1.0116\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0991 - root_mean_squared_error: 1.0484 - val_loss: 0.9791 - val_root_mean_squared_error: 0.9895\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0966 - root_mean_squared_error: 1.0472 - val_loss: 0.9814 - val_root_mean_squared_error: 0.9906\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.1010 - root_mean_squared_error: 1.0493 - val_loss: 0.9753 - val_root_mean_squared_error: 0.9876\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0945 - root_mean_squared_error: 1.0462 - val_loss: 0.9872 - val_root_mean_squared_error: 0.9936\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0915 - root_mean_squared_error: 1.0448 - val_loss: 1.0056 - val_root_mean_squared_error: 1.0028\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0929 - root_mean_squared_error: 1.0454 - val_loss: 0.9813 - val_root_mean_squared_error: 0.9906\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0933 - root_mean_squared_error: 1.0456 - val_loss: 0.9752 - val_root_mean_squared_error: 0.9875\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0962 - root_mean_squared_error: 1.0470 - val_loss: 0.9767 - val_root_mean_squared_error: 0.9883\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0967 - root_mean_squared_error: 1.0473 - val_loss: 0.9770 - val_root_mean_squared_error: 0.9884\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0931 - root_mean_squared_error: 1.0455 - val_loss: 0.9826 - val_root_mean_squared_error: 0.9912\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0955 - root_mean_squared_error: 1.0467 - val_loss: 0.9734 - val_root_mean_squared_error: 0.9866\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0924 - root_mean_squared_error: 1.0452 - val_loss: 0.9892 - val_root_mean_squared_error: 0.9946\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0940 - root_mean_squared_error: 1.0460 - val_loss: 0.9746 - val_root_mean_squared_error: 0.9872\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0943 - root_mean_squared_error: 1.0461 - val_loss: 0.9800 - val_root_mean_squared_error: 0.9899\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0904 - root_mean_squared_error: 1.0442 - val_loss: 0.9745 - val_root_mean_squared_error: 0.9872\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0883 - root_mean_squared_error: 1.0432 - val_loss: 1.0582 - val_root_mean_squared_error: 1.0287\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0946 - root_mean_squared_error: 1.0462 - val_loss: 1.0686 - val_root_mean_squared_error: 1.0337\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0958 - root_mean_squared_error: 1.0468 - val_loss: 1.0211 - val_root_mean_squared_error: 1.0105\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0916 - root_mean_squared_error: 1.0448 - val_loss: 0.9803 - val_root_mean_squared_error: 0.9901\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0887 - root_mean_squared_error: 1.0434 - val_loss: 0.9723 - val_root_mean_squared_error: 0.9860\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0963 - root_mean_squared_error: 1.0470 - val_loss: 0.9736 - val_root_mean_squared_error: 0.9867\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0906 - root_mean_squared_error: 1.0443 - val_loss: 1.0023 - val_root_mean_squared_error: 1.0012\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 1.0900 - root_mean_squared_error: 1.0440 - val_loss: 1.0400 - val_root_mean_squared_error: 1.0198\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.1009 - root_mean_squared_error: 1.0492 - val_loss: 0.9767 - val_root_mean_squared_error: 0.9883\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0905 - root_mean_squared_error: 1.0443 - val_loss: 0.9728 - val_root_mean_squared_error: 0.9863\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0976 - root_mean_squared_error: 1.0477 - val_loss: 0.9882 - val_root_mean_squared_error: 0.9941\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0972 - root_mean_squared_error: 1.0475 - val_loss: 0.9870 - val_root_mean_squared_error: 0.9935\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0882 - root_mean_squared_error: 1.0432 - val_loss: 0.9791 - val_root_mean_squared_error: 0.9895\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9791 - root_mean_squared_error: 0.9895\n",
      "16/16 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "results_ngram = run_tests(feature_dfs[1].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step\n",
      "Validation score for model: Ridge -- 0.951\n",
      "Validation score for model: Ridge with Polynomial Features -- 1.22\n",
      "Validation score for model: Lasso -- 0.951\n",
      "Validation score for model: Lasso with Polynomial Features -- 0.602\n",
      "Validation score for model: KNN -- 1.069\n",
      "Validation score for model: SVR -- 0.523\n",
      "Validation score for model: GBR -- 0.447\n",
      "Validation score for model: LGBM -- 0.432\n",
      "Validation score for model: XGB -- 0.439\n",
      "Validation score for model: NN -- 0.979\n",
      "Weighted ensemble of models for validation set: 0.432\n"
     ]
    }
   ],
   "source": [
    "changable_rs = 0\n",
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=.2, random_state=changable_rs)\n",
    "\n",
    "y_valid_preds_ngrams = []\n",
    "for model in models + [{'name': 'NN', 'preprocess': lambda X: X}]:\n",
    "    tuned_model = results_ngram[model['name']]['model']\n",
    "    y_pred = tuned_model.predict(model['preprocess'](X_valid[feature_dfs[1].columns]))\n",
    "    y_valid_preds_ngrams.append({\n",
    "        'model': model['name'], \n",
    "        'features': 'patterns', \n",
    "        'y_valid_pred': y_pred if model['name'] != 'NN' else y_pred[:,0], \n",
    "        'score': mean_squared_error(y_pred, y_valid)})\n",
    "\n",
    "weights = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "y_valid_pred_ngram = []\n",
    "for i in range(len(y_valid_preds_ngrams)):\n",
    "    sc = round(mean_squared_error(y_valid, y_valid_preds_ngrams[i]['y_valid_pred']), 3)\n",
    "    print(f'Validation score for model: {y_valid_preds_ngrams[i][\"model\"]} -- {sc}')\n",
    "for i in range(len(y_valid_preds_ngrams)):\n",
    "    vect = y_valid_preds_ngrams[i]['y_valid_pred'] * weights[i]\n",
    "    if i == 0:\n",
    "        y_valid_pred_ngram = vect\n",
    "    else:\n",
    "        y_valid_pred_ngram = y_valid_pred_ngram + vect\n",
    "sc = round(mean_squared_error(y_valid, y_valid_pred_ngram), 3)\n",
    "print(f'Weighted ensemble of models for validation set: {sc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try this with iki info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for Ridge: 0.759, Best Params: {'alpha': 0.1, 'fit_intercept': False}\n",
      "Best Score for Ridge with Polynomial Features: 1.733, Best Params: {'alpha': 10, 'fit_intercept': False}\n",
      "Best Score for Lasso: 0.758, Best Params: {'alpha': 0, 'fit_intercept': False}\n",
      "Best Score for Lasso with Polynomial Features: 0.794, Best Params: {'alpha': 0.1, 'fit_intercept': True}\n",
      "Best Score for KNN: 0.763, Best Params: {'n_neighbors': 20}\n",
      "Best Score for SVR: 0.719, Best Params: {'C': 1, 'degree': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best Score for GBR: 0.719, Best Params: {'max_depth': 2, 'n_estimators': 100, 'subsample': 0.65}\n",
      "Best Score for LGBM: 0.719, Best Params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'num_leaves': 131072, 'subsample': 0.5}\n",
      "Best Score for XGB: 0.716, Best Params: {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 1s 8ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 10ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 6ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 1s 13ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "16/16 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan\n",
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "results_iki = run_tests(feature_dfs[2].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score for model: Ridge -- 0.525\n",
      "Validation score for model: Ridge with Polynomial Features -- 0.779\n",
      "Validation score for model: Lasso -- 0.533\n",
      "Validation score for model: Lasso with Polynomial Features -- 0.548\n",
      "Validation score for model: KNN -- 0.53\n",
      "Validation score for model: SVR -- 0.508\n",
      "Validation score for model: GBR -- 0.468\n",
      "Validation score for model: LGBM -- 0.486\n",
      "Validation score for model: XGB -- 0.469\n",
      "Weighted ensemble of models for validation set: 0.486\n"
     ]
    }
   ],
   "source": [
    "changable_rs = 0\n",
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=.2, random_state=changable_rs)\n",
    "\n",
    "y_valid_preds_iki = []\n",
    "for model in models:\n",
    "    tuned_model = results_iki[model['name']]['model']\n",
    "    y_pred = tuned_model.predict(model['preprocess'](X_valid[feature_dfs[2].columns]))\n",
    "    y_valid_preds_iki.append({\n",
    "        'model': model['name'], \n",
    "        'features': 'iki', \n",
    "        'y_valid_pred': y_pred if model['name'] != 'NN' else y_pred[:,0], \n",
    "        'score': mean_squared_error(y_pred, y_valid)})\n",
    "\n",
    "weights = [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "y_valid_pred_iki = []\n",
    "for i in range(len(y_valid_preds_iki)):\n",
    "    sc = round(mean_squared_error(y_valid, y_valid_preds_iki[i]['y_valid_pred']), 3)\n",
    "    print(f'Validation score for model: {y_valid_preds_iki[i][\"model\"]} -- {sc}')\n",
    "for i in range(len(y_valid_preds_iki)):\n",
    "    vect = y_valid_preds_iki[i]['y_valid_pred'] * weights[i]\n",
    "    if i == 0:\n",
    "        y_valid_pred_iki = vect\n",
    "    else:\n",
    "        y_valid_pred_iki = y_valid_pred_iki + vect\n",
    "sc = round(mean_squared_error(y_valid, y_valid_pred_iki), 3)\n",
    "print(f'Weighted ensemble of models for validation set: {sc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the step for the aggregate info feature df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for Ridge: 0.667, Best Params: {'alpha': 0, 'fit_intercept': True}\n",
      "Best Score for Ridge with Polynomial Features: 1.687, Best Params: {'alpha': 10, 'fit_intercept': False}\n",
      "Best Score for Lasso: 0.679, Best Params: {'alpha': 0, 'fit_intercept': True}\n",
      "Best Score for Lasso with Polynomial Features: 0.775, Best Params: {'alpha': 0.1, 'fit_intercept': True}\n",
      "Best Score for KNN: 0.705, Best Params: {'n_neighbors': 20}\n",
      "Best Score for SVR: 0.666, Best Params: {'C': 1, 'degree': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best Score for GBR: 0.641, Best Params: {'max_depth': 3, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Best Score for LGBM: 0.645, Best Params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'num_leaves': 131072, 'subsample': 0.5}\n",
      "Best Score for XGB: 0.641, Best Params: {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 1s 9ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 9ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 1s 11ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 11ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 8ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "16/16 [==============================] - 0s 3ms/step - loss: nan - root_mean_squared_error: nan\n",
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "results_agg = run_tests(feature_dfs[3].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score for model: Ridge -- 0.392\n",
      "Validation score for model: Ridge with Polynomial Features -- 1.396\n",
      "Validation score for model: Lasso -- 0.412\n",
      "Validation score for model: Lasso with Polynomial Features -- 0.461\n",
      "Validation score for model: KNN -- 0.415\n",
      "Validation score for model: SVR -- 0.388\n",
      "Validation score for model: GBR -- 0.411\n",
      "Validation score for model: LGBM -- 0.418\n",
      "Validation score for model: XGB -- 0.408\n",
      "Weighted ensemble of models for validation set: 0.378\n"
     ]
    }
   ],
   "source": [
    "changable_rs = 0\n",
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=.2, random_state=changable_rs)\n",
    "\n",
    "y_valid_preds_agg = []\n",
    "for model in models:\n",
    "    tuned_model = results_agg[model['name']]['model']\n",
    "    y_pred = tuned_model.predict(model['preprocess'](X_valid[feature_dfs[3].columns]))\n",
    "    y_valid_preds_agg.append({\n",
    "        'model': model['name'], \n",
    "        'features': 'agg', \n",
    "        'y_valid_pred': y_pred if model['name'] != 'NN' else y_pred[:,0], \n",
    "        'score': mean_squared_error(y_pred, y_valid)})\n",
    "\n",
    "weights = [0.5, 0, 0, 0, 0, 0, .5, 0, 0]\n",
    "y_valid_pred_agg = []\n",
    "for i in range(len(y_valid_preds_agg)):\n",
    "    sc = round(mean_squared_error(y_valid, y_valid_preds_agg[i]['y_valid_pred']), 3)\n",
    "    print(f'Validation score for model: {y_valid_preds_agg[i][\"model\"]} -- {sc}')\n",
    "for i in range(len(y_valid_preds_agg)):\n",
    "    vect = y_valid_preds_agg[i]['y_valid_pred'] * weights[i]\n",
    "    if i == 0:\n",
    "        y_valid_pred_agg = vect\n",
    "    else:\n",
    "        y_valid_pred_agg = y_valid_pred_agg + vect\n",
    "sc = round(mean_squared_error(y_valid, y_valid_pred_agg), 3)\n",
    "print(f'Weighted ensemble of models for validation set: {sc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, other count info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for Ridge: 0.875, Best Params: {'alpha': 0, 'fit_intercept': True}\n",
      "Best Score for Ridge with Polynomial Features: 0.966, Best Params: {'alpha': 10, 'fit_intercept': False}\n",
      "Best Score for Lasso: 0.875, Best Params: {'alpha': 0, 'fit_intercept': True}\n",
      "Best Score for Lasso with Polynomial Features: 0.84, Best Params: {'alpha': 0.01, 'fit_intercept': False}\n",
      "Best Score for KNN: 0.865, Best Params: {'n_neighbors': 20}\n",
      "Best Score for SVR: 0.815, Best Params: {'C': 1, 'degree': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best Score for GBR: 0.761, Best Params: {'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best Score for LGBM: 0.762, Best Params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'num_leaves': 131072, 'subsample': 0.5}\n",
      "Best Score for XGB: 0.759, Best Params: {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.65}\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 1s 9ms/step - loss: 3.8100 - root_mean_squared_error: 1.9519 - val_loss: 1.0917 - val_root_mean_squared_error: 1.0449\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0912 - root_mean_squared_error: 1.0446 - val_loss: 1.0496 - val_root_mean_squared_error: 1.0245\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0917 - root_mean_squared_error: 1.0449 - val_loss: 1.0318 - val_root_mean_squared_error: 1.0158\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0874 - root_mean_squared_error: 1.0428 - val_loss: 1.0129 - val_root_mean_squared_error: 1.0064\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0919 - root_mean_squared_error: 1.0449 - val_loss: 1.0177 - val_root_mean_squared_error: 1.0088\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0826 - root_mean_squared_error: 1.0405 - val_loss: 1.1430 - val_root_mean_squared_error: 1.0691\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0941 - root_mean_squared_error: 1.0460 - val_loss: 1.0442 - val_root_mean_squared_error: 1.0219\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0881 - root_mean_squared_error: 1.0431 - val_loss: 1.0702 - val_root_mean_squared_error: 1.0345\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0869 - root_mean_squared_error: 1.0425 - val_loss: 1.0306 - val_root_mean_squared_error: 1.0152\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0861 - root_mean_squared_error: 1.0422 - val_loss: 1.1325 - val_root_mean_squared_error: 1.0642\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 1.0829 - root_mean_squared_error: 1.0406 - val_loss: 1.0046 - val_root_mean_squared_error: 1.0023\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0809 - root_mean_squared_error: 1.0397 - val_loss: 1.0542 - val_root_mean_squared_error: 1.0268\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 1.0788 - root_mean_squared_error: 1.0387 - val_loss: 0.9999 - val_root_mean_squared_error: 0.9999\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0770 - root_mean_squared_error: 1.0378 - val_loss: 1.0004 - val_root_mean_squared_error: 1.0002\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0755 - root_mean_squared_error: 1.0371 - val_loss: 0.9969 - val_root_mean_squared_error: 0.9984\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0772 - root_mean_squared_error: 1.0379 - val_loss: 1.0794 - val_root_mean_squared_error: 1.0389\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0783 - root_mean_squared_error: 1.0384 - val_loss: 0.9931 - val_root_mean_squared_error: 0.9966\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 1.0749 - root_mean_squared_error: 1.0368 - val_loss: 0.9977 - val_root_mean_squared_error: 0.9988\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0753 - root_mean_squared_error: 1.0370 - val_loss: 1.0090 - val_root_mean_squared_error: 1.0045\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0724 - root_mean_squared_error: 1.0356 - val_loss: 1.1880 - val_root_mean_squared_error: 1.0899\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0778 - root_mean_squared_error: 1.0381 - val_loss: 1.1196 - val_root_mean_squared_error: 1.0581\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0642 - root_mean_squared_error: 1.0316 - val_loss: 0.9878 - val_root_mean_squared_error: 0.9939\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0692 - root_mean_squared_error: 1.0340 - val_loss: 0.9856 - val_root_mean_squared_error: 0.9928\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 1.0749 - root_mean_squared_error: 1.0368 - val_loss: 0.9840 - val_root_mean_squared_error: 0.9920\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0645 - root_mean_squared_error: 1.0317 - val_loss: 0.9833 - val_root_mean_squared_error: 0.9916\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0683 - root_mean_squared_error: 1.0336 - val_loss: 0.9810 - val_root_mean_squared_error: 0.9905\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0665 - root_mean_squared_error: 1.0327 - val_loss: 0.9860 - val_root_mean_squared_error: 0.9930\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0662 - root_mean_squared_error: 1.0326 - val_loss: 1.0268 - val_root_mean_squared_error: 1.0133\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0697 - root_mean_squared_error: 1.0343 - val_loss: 0.9937 - val_root_mean_squared_error: 0.9968\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 1.0565 - root_mean_squared_error: 1.0279 - val_loss: 0.9824 - val_root_mean_squared_error: 0.9912\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.9824 - root_mean_squared_error: 0.9912\n",
      "16/16 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "results_other = run_tests(feature_dfs[4].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 6ms/step\n",
      "Validation score for model: Ridge -- 1.038\n",
      "Validation score for model: Ridge with Polynomial Features -- 2.176\n",
      "Validation score for model: Lasso -- 1.038\n",
      "Validation score for model: Lasso with Polynomial Features -- 0.787\n",
      "Validation score for model: KNN -- 0.691\n",
      "Validation score for model: SVR -- 0.636\n",
      "Validation score for model: GBR -- 0.571\n",
      "Validation score for model: LGBM -- 0.588\n",
      "Validation score for model: XGB -- 0.567\n",
      "Validation score for model: NN -- 0.939\n",
      "Weighted ensemble of models for validation set: 0.564\n"
     ]
    }
   ],
   "source": [
    "changable_rs = 0\n",
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=.2, random_state=changable_rs)\n",
    "\n",
    "y_valid_preds_other = []\n",
    "for model in models + [{'name': 'NN', 'preprocess': lambda X: X}]:\n",
    "    tuned_model = results_other[model['name']]['model']\n",
    "    y_pred = tuned_model.predict(model['preprocess'](X_valid[feature_dfs[4].columns]))\n",
    "    y_valid_preds_other.append({\n",
    "        'model': model['name'], \n",
    "        'features': 'other count info', \n",
    "        'y_valid_pred': y_pred if model['name'] != 'NN' else y_pred[:,0], \n",
    "        'score': mean_squared_error(y_pred, y_valid)})\n",
    "\n",
    "weights = [0, 0, 0, 0, 0, 0, .5, 0, .5, 0]\n",
    "y_valid_pred_other = []\n",
    "for i in range(len(y_valid_preds_other)):\n",
    "    sc = round(mean_squared_error(y_valid, y_valid_preds_other[i]['y_valid_pred']), 3)\n",
    "    print(f'Validation score for model: {y_valid_preds_other[i][\"model\"]} -- {sc}')\n",
    "for i in range(len(y_valid_preds_other)):\n",
    "    vect = y_valid_preds_other[i]['y_valid_pred'] * weights[i]\n",
    "    if i == 0:\n",
    "        y_valid_pred_other = vect\n",
    "    else:\n",
    "        y_valid_pred_other = y_valid_pred_other + vect\n",
    "sc = round(mean_squared_error(y_valid, y_valid_pred_other), 3)\n",
    "print(f'Weighted ensemble of models for validation set: {sc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run tests for gap info df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for Ridge: 0.722, Best Params: {'alpha': 10, 'fit_intercept': True}\n",
      "Best Score for Ridge with Polynomial Features: 1.201, Best Params: {'alpha': 10, 'fit_intercept': True}\n",
      "Best Score for Lasso: 0.718, Best Params: {'alpha': 0.01, 'fit_intercept': True}\n",
      "Best Score for Lasso with Polynomial Features: 0.704, Best Params: {'alpha': 0.01, 'fit_intercept': True}\n",
      "Best Score for KNN: 0.712, Best Params: {'n_neighbors': 20}\n",
      "Best Score for SVR: 0.69, Best Params: {'C': 1, 'degree': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best Score for GBR: 0.673, Best Params: {'max_depth': 3, 'n_estimators': 50, 'subsample': 0.65}\n",
      "Best Score for LGBM: 0.672, Best Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'num_leaves': 131072, 'subsample': 0.5}\n",
      "Best Score for XGB: 0.668, Best Params: {'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 1s 8ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 5ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 4ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
      "16/16 [==============================] - 0s 3ms/step - loss: nan - root_mean_squared_error: nan\n",
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "results_gap = run_tests(feature_dfs[5].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score for model: Ridge -- 0.468\n",
      "Validation score for model: Ridge with Polynomial Features -- 0.752\n",
      "Validation score for model: Lasso -- 0.469\n",
      "Validation score for model: Lasso with Polynomial Features -- 0.468\n",
      "Validation score for model: KNN -- 0.479\n",
      "Validation score for model: SVR -- 0.455\n",
      "Validation score for model: GBR -- 0.414\n",
      "Validation score for model: LGBM -- 0.425\n",
      "Validation score for model: XGB -- 0.411\n",
      "Weighted ensemble of models for validation set: 0.411\n"
     ]
    }
   ],
   "source": [
    "changable_rs = 0\n",
    "X, y = combined_feats, train_scores['score']\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=.2, random_state=changable_rs)\n",
    "\n",
    "y_valid_preds_gap = []\n",
    "for model in models:\n",
    "    tuned_model = results_gap[model['name']]['model']\n",
    "    y_pred = tuned_model.predict(model['preprocess'](X_valid[feature_dfs[5].columns]))\n",
    "    y_valid_preds_gap.append({\n",
    "        'model': model['name'], \n",
    "        'features': 'other count info', \n",
    "        'y_valid_pred': y_pred if model['name'] != 'NN' else y_pred[:,0], \n",
    "        'score': mean_squared_error(y_pred, y_valid)})\n",
    "\n",
    "weights = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "y_valid_pred_gap = []\n",
    "for i in range(len(y_valid_preds_gap)):\n",
    "    sc = round(mean_squared_error(y_valid, y_valid_preds_gap[i]['y_valid_pred']), 3)\n",
    "    print(f'Validation score for model: {y_valid_preds_gap[i][\"model\"]} -- {sc}')\n",
    "for i in range(len(y_valid_preds_gap)):\n",
    "    vect = y_valid_preds_gap[i]['y_valid_pred'] * weights[i]\n",
    "    if i == 0:\n",
    "        y_valid_pred_gap = vect\n",
    "    else:\n",
    "        y_valid_pred_gap = y_valid_pred_gap + vect\n",
    "sc = round(mean_squared_error(y_valid, y_valid_pred_gap), 3)\n",
    "print(f'Weighted ensemble of models for validation set: {sc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to combine the best from all these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for pattern model: 0.74\n",
      "Best score for ngram tdidf model: 0.432\n",
      "Best score for iki model: 0.486\n",
      "Best score for agg model: 0.378\n",
      "Best score for other model: 0.564\n",
      "Best score for gap model: 0.411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemble_preds = [\n",
    "    (y_valid_pred_pattern, 'pattern'),\n",
    "    (y_valid_pred_ngram, 'ngram tdidf'),\n",
    "    (y_valid_pred_iki, 'iki'),\n",
    "    (y_valid_pred_agg, 'agg'),\n",
    "    (y_valid_pred_other, 'other'),\n",
    "    (y_valid_pred_gap, 'gap')\n",
    "]\n",
    "for i in range(len(ensemble_preds)):\n",
    "    sc = round(mean_squared_error(y_valid, ensemble_preds[i][0]), 3)\n",
    "    print(f'Best score for {ensemble_preds[i][1]} model: {sc}')\n",
    "scores = []\n",
    "iotas = np.arange(0, 1.01, .01)\n",
    "for iota in iotas:\n",
    "    scores.append(mean_squared_error(y_valid, y_valid_pred_agg * iota + y_valid_pred_gap * (1 - iota)))\n",
    "ens = y_valid_pred_agg * .65 + y_valid_pred_gap * .35\n",
    "scores = []\n",
    "for iota in iotas:\n",
    "    scores.append(mean_squared_error(y_valid, ens * iota + y_valid_pred_ngram * (1 - iota)))\n",
    "ens = ens * .7 + y_valid_pred_ngram * .3\n",
    "scores = []\n",
    "for iota in iotas:\n",
    "    scores.append(mean_squared_error(y_valid, ens * iota + y_valid_pred_other * (1 - iota)))\n",
    "plt.scatter(iotas, scores, s=3)\n",
    "ens = ens * .9 + y_valid_pred_other * .1\n",
    "round(mean_squared_error(y_valid, ens), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
