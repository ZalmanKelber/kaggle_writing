{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataFrames\n",
    "train_logs = pd.read_csv('input/train_logs.csv')\n",
    "test_logs = pd.read_csv('input/test_logs.csv')\n",
    "train_scores = pd.read_csv('input/train_scores.csv')\n",
    "train_essays = pd.read_csv('output/train_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish some variables and functions for testing and evaluating\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract several different kinds of features and add them to separate dataframes\n",
    "1. Word frequency (using TF-IDF scores) from the \"words\" of the reconstructed essays\n",
    "2. Aggregations of events, text changes, &c along with event index gaps\n",
    "3. Gap info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with \"word\" analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_tdidf(essay_df):\n",
    "    # Get counts of all 1, 2, and 3-grams\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "    X_tokenizer_train = count_vectorizer.fit_transform(essay_df['essay']).todense()\n",
    "    group_names = count_vectorizer.get_feature_names_out()\n",
    "    # Create a matrix of 1,0 values to keep track of which grams appear in which essays at least once\n",
    "    X_tokenizer_reduced = X_tokenizer_train.copy()\n",
    "    for i in reversed(range(X_tokenizer_reduced.shape[1])):\n",
    "        X_tokenizer_reduced[:,i] = np.where(X_tokenizer_reduced[:,i] > 0, 1, 0)\n",
    "\n",
    "    # Create tdidf DataFrame\n",
    "    col_names = [f'word_group_{group_names[i]}' for i in range(X_tokenizer_train.shape[1])]\n",
    "    X_tokenizer_final = pd.DataFrame(X_tokenizer_train.copy(), columns=col_names)\n",
    "    idfs = []\n",
    "    for i in range(X_tokenizer_final.shape[1]):\n",
    "        idfs.append(np.log(X_tokenizer_final.shape[0] / np.sum(X_tokenizer_reduced[:,i])))\n",
    "    def compute_tfidf(row):\n",
    "        sum = np.sum([row[col] for col in col_names])\n",
    "        for i in range(X_tokenizer_train.shape[1]):\n",
    "            row[f'word_group_{group_names[i]}'] = row[f'word_group_{group_names[i]}'] * idfs[i] / sum\n",
    "        return row\n",
    "    X_tokenizer_final = X_tokenizer_final.apply(compute_tfidf, axis = 1)\n",
    "    return X_tokenizer_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_group__qqqq</th>\n",
       "      <th>word_group__qqqq qqqqqqqqq</th>\n",
       "      <th>word_group__qqqq qqqqqqqqq qqqqqqqqqqq__</th>\n",
       "      <th>word_group_qq</th>\n",
       "      <th>word_group_qq qq</th>\n",
       "      <th>word_group_qq qq qq</th>\n",
       "      <th>word_group_qq qq qqq</th>\n",
       "      <th>word_group_qq qq qqqq</th>\n",
       "      <th>word_group_qq qq qqqqq</th>\n",
       "      <th>word_group_qq qq qqqqqq</th>\n",
       "      <th>...</th>\n",
       "      <th>word_group_qqqqqâ qqqqqq qq</th>\n",
       "      <th>word_group_qåäqqqqqqqqqq</th>\n",
       "      <th>word_group_qåäqqqqqqqqqq qqq</th>\n",
       "      <th>word_group_qåäqqqqqqqqqq qqq qqqqq</th>\n",
       "      <th>word_group_äq</th>\n",
       "      <th>word_group_äq qq</th>\n",
       "      <th>word_group_äq qq qq</th>\n",
       "      <th>word_group_ëqqqqqqqqq</th>\n",
       "      <th>word_group_ëqqqqqqqqq qqq</th>\n",
       "      <th>word_group_ëqqqqqqqqq qqq qqqqqqq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_group__qqqq  word_group__qqqq qqqqqqqqq  \\\n",
       "0               0.0                         0.0   \n",
       "1               0.0                         0.0   \n",
       "2               0.0                         0.0   \n",
       "\n",
       "   word_group__qqqq qqqqqqqqq qqqqqqqqqqq__  word_group_qq  word_group_qq qq  \\\n",
       "0                                       0.0            0.0          0.000152   \n",
       "1                                       0.0            0.0          0.000141   \n",
       "2                                       0.0            0.0          0.000084   \n",
       "\n",
       "   word_group_qq qq qq  word_group_qq qq qqq  word_group_qq qq qqqq  \\\n",
       "0             0.000928              0.000472               0.001125   \n",
       "1             0.000753              0.000767               0.000609   \n",
       "2             0.000512              0.000260               0.000207   \n",
       "\n",
       "   word_group_qq qq qqqqq  word_group_qq qq qqqqqq  ...  \\\n",
       "0                0.000711                 0.000770  ...   \n",
       "1                0.000000                 0.000000  ...   \n",
       "2                0.000392                 0.000849  ...   \n",
       "\n",
       "   word_group_qqqqqâ qqqqqq qq  word_group_qåäqqqqqqqqqq  \\\n",
       "0                          0.0                       0.0   \n",
       "1                          0.0                       0.0   \n",
       "2                          0.0                       0.0   \n",
       "\n",
       "   word_group_qåäqqqqqqqqqq qqq  word_group_qåäqqqqqqqqqq qqq qqqqq  \\\n",
       "0                           0.0                                 0.0   \n",
       "1                           0.0                                 0.0   \n",
       "2                           0.0                                 0.0   \n",
       "\n",
       "   word_group_äq  word_group_äq qq  word_group_äq qq qq  \\\n",
       "0            0.0               0.0                  0.0   \n",
       "1            0.0               0.0                  0.0   \n",
       "2            0.0               0.0                  0.0   \n",
       "\n",
       "   word_group_ëqqqqqqqqq  word_group_ëqqqqqqqqq qqq  \\\n",
       "0                    0.0                        0.0   \n",
       "1                    0.0                        0.0   \n",
       "2                    0.0                        0.0   \n",
       "\n",
       "   word_group_ëqqqqqqqqq qqq qqqqqqq  \n",
       "0                                0.0  \n",
       "1                                0.0  \n",
       "2                                0.0  \n",
       "\n",
       "[3 rows x 3395 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_tdidf = get_tdidf(train_essays)\n",
    "train_ngram_tdidf.to_csv('output/train_ngram_tdidf.csv')\n",
    "train_ngram_tdidf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.4011591562826782\n",
      "MSQE: validation set\n",
      "0.7049318672765424\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = train_ngram_tdidf, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate gap info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longest_iki</th>\n",
       "      <th>median_iki</th>\n",
       "      <th>mean_iki</th>\n",
       "      <th>initial_pause</th>\n",
       "      <th>start_time</th>\n",
       "      <th>100ms_run_count</th>\n",
       "      <th>100ms_long_run_count</th>\n",
       "      <th>100ms_max_run</th>\n",
       "      <th>portion_ikis_under_100ms</th>\n",
       "      <th>300ms_run_count</th>\n",
       "      <th>...</th>\n",
       "      <th>10000ms_max_run</th>\n",
       "      <th>portion_ikis_under_10000ms</th>\n",
       "      <th>30000ms_run_count</th>\n",
       "      <th>30000ms_long_run_count</th>\n",
       "      <th>30000ms_max_run</th>\n",
       "      <th>portion_ikis_under_30000ms</th>\n",
       "      <th>60000ms_run_count</th>\n",
       "      <th>60000ms_long_run_count</th>\n",
       "      <th>60000ms_max_run</th>\n",
       "      <th>portion_ikis_under_60000ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.136</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.610944</td>\n",
       "      <td>101.609</td>\n",
       "      <td>4526</td>\n",
       "      <td>1618</td>\n",
       "      <td>245.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.686742</td>\n",
       "      <td>2065</td>\n",
       "      <td>...</td>\n",
       "      <td>389</td>\n",
       "      <td>0.991396</td>\n",
       "      <td>2549</td>\n",
       "      <td>2549</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.998045</td>\n",
       "      <td>2552</td>\n",
       "      <td>2552</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.999218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145.899</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.620108</td>\n",
       "      <td>1.696</td>\n",
       "      <td>30623</td>\n",
       "      <td>1603</td>\n",
       "      <td>640.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.689487</td>\n",
       "      <td>2047</td>\n",
       "      <td>...</td>\n",
       "      <td>770</td>\n",
       "      <td>0.988183</td>\n",
       "      <td>2443</td>\n",
       "      <td>2431</td>\n",
       "      <td>1459</td>\n",
       "      <td>0.996740</td>\n",
       "      <td>2449</td>\n",
       "      <td>2449</td>\n",
       "      <td>2321</td>\n",
       "      <td>0.998778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153.886</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.360506</td>\n",
       "      <td>16.736</td>\n",
       "      <td>4441</td>\n",
       "      <td>3333</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.821809</td>\n",
       "      <td>3794</td>\n",
       "      <td>...</td>\n",
       "      <td>681</td>\n",
       "      <td>0.995164</td>\n",
       "      <td>4123</td>\n",
       "      <td>4123</td>\n",
       "      <td>766</td>\n",
       "      <td>0.997340</td>\n",
       "      <td>4130</td>\n",
       "      <td>4130</td>\n",
       "      <td>2528</td>\n",
       "      <td>0.999033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   longest_iki  median_iki  mean_iki  initial_pause  start_time  \\\n",
       "0      154.136       0.062  0.610944        101.609        4526   \n",
       "1      145.899       0.061  0.620108          1.696       30623   \n",
       "2      153.886       0.040  0.360506         16.736        4441   \n",
       "\n",
       "   100ms_run_count  100ms_long_run_count  100ms_max_run  \\\n",
       "0             1618                 245.0             24   \n",
       "1             1603                 640.0             61   \n",
       "2             3333                2300.0             39   \n",
       "\n",
       "   portion_ikis_under_100ms  300ms_run_count  ...  10000ms_max_run  \\\n",
       "0                  0.686742             2065  ...              389   \n",
       "1                  0.689487             2047  ...              770   \n",
       "2                  0.821809             3794  ...              681   \n",
       "\n",
       "   portion_ikis_under_10000ms  30000ms_run_count  30000ms_long_run_count  \\\n",
       "0                    0.991396               2549                    2549   \n",
       "1                    0.988183               2443                    2431   \n",
       "2                    0.995164               4123                    4123   \n",
       "\n",
       "   30000ms_max_run  portion_ikis_under_30000ms  60000ms_run_count  \\\n",
       "0             1338                    0.998045               2552   \n",
       "1             1459                    0.996740               2449   \n",
       "2              766                    0.997340               4130   \n",
       "\n",
       "   60000ms_long_run_count  60000ms_max_run  portion_ikis_under_60000ms  \n",
       "0                    2552             2500                    0.999218  \n",
       "1                    2449             2321                    0.998778  \n",
       "2                    4130             2528                    0.999033  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_iki_info(logs_df):\n",
    "    logs_df['prev_up_time'] = logs_df.groupby('id')['up_time'].shift(1).fillna(logs_df['down_time'])\n",
    "    logs_df['inter_keystroke_intvl'] = (abs(logs_df['down_time'] - logs_df['prev_up_time'])) / 1000\n",
    "    group = logs_df.groupby('id')['inter_keystroke_intvl']\n",
    "    total_ikis = group.count()\n",
    "   \n",
    "    data =  pd.DataFrame({\n",
    "        'id': logs_df['id'].unique(),\n",
    "        'longest_iki': group.max(),\n",
    "        'median_iki': group.median(),\n",
    "        'mean_iki': group.mean(),\n",
    "        'initial_pause': group.apply(lambda ikis: max(ikis.values[:10])),\n",
    "        'start_time': logs_df.groupby('id')['down_time'].first()\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    ms_thresholds = [100, 300, 500, 1000, 2000, 10000, 30000, 60000]\n",
    "    for ms in ms_thresholds:\n",
    "        def get_runs(ikis):\n",
    "            runs = []\n",
    "            curr_run = 0\n",
    "            for i, val in enumerate(ikis.values[1:]):\n",
    "                if val > ms / 1000 or i == len(ikis.values) - 2:\n",
    "                    if curr_run >= 2:\n",
    "                        runs.append(curr_run)\n",
    "                    curr_run = 0\n",
    "                else:\n",
    "                    curr_run += 1\n",
    "            return runs\n",
    "        runs_series = group.apply(get_runs)\n",
    "        # count total number of ikis in runs (not number of runs)\n",
    "        data[f'{ms}ms_run_count'] = runs_series.apply(lambda runs: np.sum(runs)).values\n",
    "        data[f'{ms}ms_long_run_count'] = runs_series.apply(lambda runs: np.sum([n for n in runs if n >= 10])).values\n",
    "        # count longest run\n",
    "        data[f'{ms}ms_max_run'] = runs_series.apply(lambda runs: max(runs)).values\n",
    "        # count portion of ikis under threshold\n",
    "        num_ikis = group.apply(lambda x: len([n for n in x.values if n < ms / 1000])).values\n",
    "        data[f'portion_ikis_under_{ms}ms'] = num_ikis / total_ikis.values\n",
    "    return data.drop('id', axis=1)\n",
    "iki_info = get_iki_info(train_logs[['id', 'up_time', 'down_time']])\n",
    "iki_info.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.5364813295373154\n",
      "MSQE: validation set\n",
      "0.7253544112320706\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = iki_info, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract specific fragments in the writing that may indicate either close attention to detail, complex grammar/punctuation/style or, alternatively, carelessness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern_0</th>\n",
       "      <th>pattern_1</th>\n",
       "      <th>pattern_2</th>\n",
       "      <th>pattern_3</th>\n",
       "      <th>pattern_4</th>\n",
       "      <th>pattern_5</th>\n",
       "      <th>pattern_6</th>\n",
       "      <th>pattern_7</th>\n",
       "      <th>pattern_8</th>\n",
       "      <th>pattern_9</th>\n",
       "      <th>pattern_10</th>\n",
       "      <th>pattern_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pattern_0  pattern_1  pattern_2  pattern_3  pattern_4  pattern_5  \\\n",
       "0         10          0          0          0          0          0   \n",
       "1          0          0          0          0          0          1   \n",
       "\n",
       "   pattern_6  pattern_7  pattern_8  pattern_9  pattern_10  pattern_11  \n",
       "0          1          0          1          1           1           1  \n",
       "1          2          0          2          1           4           0  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "patterns = [\n",
    "    # good patterns:\n",
    "    r'[\\.|\\?|\\!] {2}[^ ]', # two spaces after period\n",
    "    r'[q|\\'|\\\"]--[q|\\'|\\\"]', # em-dash\n",
    "    r'[q|\\'|\\\"] -- [q|\\'|\\\"]', # AP style guide em-dash\n",
    "    r'[q|\\'|\\\"]--[q|\\'|\\\"][^\\.]+[q|\\'|\\\"]--[q|\\'|\\\"][^\\.]', # em-dash-separated sub-clause within sentence\n",
    "    r'[q|\\'|\\\"] -- [q|\\'|\\\"][^\\.]+[q|\\'|\\\"] -- [q|\\'|\\\"][^\\.]',\n",
    "    r'q-q', # hyphenated words\n",
    "    r'q\\'', # posessives or contraction\n",
    "    r'\\.\\.\\.', # elipses\n",
    "    # bad patterns:\n",
    "    r'[q|,|\\'|\\\"|-]  +[q|,|\\'|\\\"|-]', # two or more spaces in middle of sentence\n",
    "    r'[ |\\n][\\.|\\?|\\!|,|;|:|\\(|\\)|\\'][ |\\n]', # punctuation surrounded by whitespace\n",
    "    r' \\n', # unnecessary trailing space before line break\n",
    "    r'q\\n' # word character immediately followed by linebreak\n",
    "]\n",
    "\n",
    "def extract_patterns(essays_df):\n",
    "   \n",
    "    group = essays_df.groupby('id')['essay']\n",
    "\n",
    "    data =  pd.DataFrame({\n",
    "        'id': essays_df['id'].unique(),\n",
    "    }).reset_index(drop=True)\n",
    "    def apply_pattern_to_group(pattern):\n",
    "        def find_pattern(g):\n",
    "            essay = g.values[0]\n",
    "            m = re.findall(pattern, essay)\n",
    "            return len(m)\n",
    "        return find_pattern\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        col = f'pattern_{i}'\n",
    "        data[col] = group.apply(apply_pattern_to_group(pattern)).values\n",
    "    return data.drop('id', axis=1)\n",
    "pattern_info = extract_patterns(train_essays)\n",
    "pattern_info.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.8011612792925569\n",
      "MSQE: validation set\n",
      "0.8202367315364079\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = pattern_info, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2471, 101)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "def iqr(x):\n",
    "    return x.quantile(0.75) - x.quantile(.25)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, iqr, 'skew', pd.Series.kurtosis, 'sum']\n",
    "\n",
    "def analyze_sentences(essay_df):\n",
    "    sen_df = pd.DataFrame()\n",
    "    sen_df['id'] = essay_df['id']\n",
    "    sen_df['sentence'] = essay_df['essay'].apply(lambda x: re.split(r'\\.|\\?|\\!|\\n',str(x)))\n",
    "    sen_df = sen_df.explode('sentence')\n",
    "    sen_df['sentence'] = sen_df['sentence'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    sen_df['snt_len'] = sen_df['sentence'].apply(lambda x: len(x))\n",
    "    sen_df['snt_q_count'] = sen_df['sentence'].apply(lambda x: x.count('q'))\n",
    "    sen_df = sen_df[sen_df['snt_q_count'] > 0]\n",
    "    sen_df['snt_word_count'] = sen_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "    data =  data =  pd.DataFrame({\n",
    "        'id': essay_df['id'].unique(),\n",
    "    }).reset_index(drop=True)\n",
    "    for cat in ['snt_len', 'snt_q_count', 'snt_word_count']:\n",
    "        aggs = AGGREGATIONS if cat == 'snt_len' else AGGREGATIONS[1:]\n",
    "        for agg in aggs:\n",
    "            agg_name = str(agg)\n",
    "            if agg_name[0] == '<':\n",
    "                agg_name = agg_name.split(' ')[1]\n",
    "            col = f'{cat}__{agg_name}'\n",
    "            data[col] = sen_df.groupby('id')[cat].agg(agg).values\n",
    "    return data.rename(columns={'snt_len__count':'sentence__count'}).drop('id', axis=1)\n",
    "\n",
    "def analyze_paragraphs(essay_df):\n",
    "    par_df = pd.DataFrame()\n",
    "    par_df['id'] = essay_df['id']\n",
    "    par_df['p'] = essay_df['essay'].apply(lambda x: str(x).split('\\n'))\n",
    "    par_df = par_df.explode('p')\n",
    "    for ch in ['.', '(', ')', ',', '\\'', '\"', ':', ';', '?', '!', ' -', '--']:\n",
    "        par_df['p'] = par_df['p'].apply(lambda x: x.replace(ch,'').strip())\n",
    "    par_df['par_len'] = par_df['p'].apply(lambda x: len(x))\n",
    "    par_df['par_q_count'] = par_df['p'].apply(lambda x: x.count('q'))\n",
    "    par_df = par_df[par_df['par_q_count'] > 0]\n",
    "    par_df['par_word_count'] = par_df['p'].apply(lambda x: len(x.split(' ')))\n",
    "    data =  data =  pd.DataFrame({\n",
    "        'id': essay_df['id'].unique(),\n",
    "    }).reset_index(drop=True)\n",
    "    for cat in ['par_len', 'par_q_count', 'par_word_count']:\n",
    "        aggs = AGGREGATIONS if cat == 'par_len' else AGGREGATIONS[1:]\n",
    "        for agg in aggs:\n",
    "            agg_name = str(agg)\n",
    "            if agg_name[0] == '<':\n",
    "                agg_name = agg_name.split(' ')[1]\n",
    "            col = f'{cat}__{agg_name}'\n",
    "            data[col] = par_df.groupby('id')[cat].agg(agg).values\n",
    "    return data.rename(columns={'par_len__count':'paragraph__count'}).drop('id', axis=1)\n",
    "\n",
    "def analyze_words(essay_df):\n",
    "    word_df = pd.DataFrame()\n",
    "    word_df['id'] = essay_df['id']\n",
    "    word_df['word'] = essay_df['essay'].apply(lambda x: str(x).split())\n",
    "    word_df = word_df.explode('word')\n",
    "    word_df['word_q_count'] = word_df['word'].apply(lambda x: x.count('q'))\n",
    "    word_df = word_df[word_df['word_q_count'] > 0]\n",
    "    data =  data =  pd.DataFrame({\n",
    "        'id': essay_df['id'].unique(),\n",
    "    }).reset_index(drop=True)\n",
    "    for agg in AGGREGATIONS:\n",
    "        agg_name = str(agg)\n",
    "        if agg_name[0] == '<':\n",
    "            agg_name = agg_name.split(' ')[1]\n",
    "        col = f'word_q_count__{agg_name}'\n",
    "        data[col] = word_df.groupby('id')['word_q_count'].agg(agg).values\n",
    "    return data.rename(columns={'word_q_count__count':'word__count'}).drop('id', axis=1)\n",
    "\n",
    "agg_info = pd.concat([\n",
    "    analyze_sentences(train_essays), \n",
    "    analyze_paragraphs(train_essays), \n",
    "    analyze_words(train_essays)\n",
    "    ], axis=1).fillna(0)\n",
    "agg_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.45476942843953505\n",
      "MSQE: validation set\n",
      "0.6516732347351654\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = agg_info, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get count data for down events, text changes and activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste'], dtype='object', name='activity'),\n",
       " Index(['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick',\n",
       "        'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', ''',\n",
       "        'Delete', 'Unidentified', 'Control', '\"', '-', '?', ';', '=', 'Tab',\n",
       "        '/', 'Rightclick', ':', '(', ')', '\\', 'ContextMenu', 'End', '!',\n",
       "        'Meta', 'Alt', '[', 'c', 'v', 'NumLock', 'Insert', 'Home', 'z',\n",
       "        'AudioVolumeDown', 'F2', 'a', 'x', 'AudioVolumeUp', '$', '>', ']', '*'],\n",
       "       dtype='object', name='down_event'),\n",
       " Index([' ', 'NoChange', '\\n', '. '], dtype='object', name='text_change'))"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_events = train_logs.groupby('down_event')['event_id'].agg('count').sort_values(ascending=False).index[:50]\n",
    "voc = [tc for tc in train_logs.text_change.unique() if 'q' not in tc and '=>' not in tc and tc not in down_events]\n",
    "text_changes = train_logs[train_logs.text_change.isin(voc)].groupby('text_change')['event_id'].agg('count').sort_values(ascending=False).index[:4]\n",
    "activities = train_logs.groupby('activity')['event_id'].agg('count').sort_values(ascending=False).index[:5]\n",
    "activities, down_events, text_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(logs_df):\n",
    "    data = pd.DataFrame()\n",
    "    data['id'] = logs_df['id'].unique().tolist()\n",
    "    g_total_events = logs_df.groupby('id')['event_id'].agg('count').values\n",
    "    for event in down_events:\n",
    "        d = logs_df.groupby('id')['down_event'].value_counts().unstack(fill_value=0)\n",
    "        if event in d.columns:\n",
    "            g = d.loc[:,event].tolist()\n",
    "            data[f'down_event_{event}_count'] = g / g_total_events\n",
    "    for text_change in text_changes:\n",
    "        text_change = text_change if text_change != '. ' else '.'\n",
    "        d = logs_df.groupby('id')['text_change'].value_counts().unstack(fill_value=0)\n",
    "        if text_change in d.columns:\n",
    "            g = d.loc[:,text_change].tolist()\n",
    "            data[f'text_change_{text_change}_count'] = g / g_total_events\n",
    "    for activity in activities:\n",
    "        d = logs_df.groupby('id')['activity'].value_counts().unstack(fill_value=0)\n",
    "        if activity in d.columns:\n",
    "            g = d.loc[:,activity].tolist()\n",
    "            data[f'activity_{activity}_count'] = g / g_total_events\n",
    "    return data.drop('id', axis=1)\n",
    "other_counts_info = get_counts(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.5727571744193612\n",
      "MSQE: validation set\n",
      "0.7224463327320648\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = other_counts_info, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now examine changes in word count, time and cursor position after different numbers of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gap_2_down_time__mean</th>\n",
       "      <th>gap_2_down_time__std</th>\n",
       "      <th>gap_2_down_time__min</th>\n",
       "      <th>gap_2_down_time__max</th>\n",
       "      <th>gap_2_down_time__first</th>\n",
       "      <th>gap_2_down_time__last</th>\n",
       "      <th>gap_2_down_time__sem</th>\n",
       "      <th>gap_2_down_time__q1</th>\n",
       "      <th>gap_2_down_time__median</th>\n",
       "      <th>gap_2_down_time__q3</th>\n",
       "      <th>...</th>\n",
       "      <th>gap_500_word_count__first</th>\n",
       "      <th>gap_500_word_count__last</th>\n",
       "      <th>gap_500_word_count__sem</th>\n",
       "      <th>gap_500_word_count__q1</th>\n",
       "      <th>gap_500_word_count__median</th>\n",
       "      <th>gap_500_word_count__q3</th>\n",
       "      <th>gap_500_word_count__iqr</th>\n",
       "      <th>gap_500_word_count__skew</th>\n",
       "      <th>gap_500_word_count__Series.kurt</th>\n",
       "      <th>gap_500_word_count__sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1399.497065</td>\n",
       "      <td>6198.512184</td>\n",
       "      <td>46.0</td>\n",
       "      <td>162229.0</td>\n",
       "      <td>102045.0</td>\n",
       "      <td>19815.0</td>\n",
       "      <td>122.628667</td>\n",
       "      <td>250.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>674.5</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.214883</td>\n",
       "      <td>42.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-0.246747</td>\n",
       "      <td>-1.263949</td>\n",
       "      <td>106495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433.109706</td>\n",
       "      <td>7173.079166</td>\n",
       "      <td>53.0</td>\n",
       "      <td>155377.0</td>\n",
       "      <td>2083.0</td>\n",
       "      <td>115062.0</td>\n",
       "      <td>144.858969</td>\n",
       "      <td>256.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.287947</td>\n",
       "      <td>62.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.137884</td>\n",
       "      <td>-0.621870</td>\n",
       "      <td>135714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850.318820</td>\n",
       "      <td>5817.295774</td>\n",
       "      <td>67.0</td>\n",
       "      <td>154879.0</td>\n",
       "      <td>16894.0</td>\n",
       "      <td>1602.0</td>\n",
       "      <td>90.476524</td>\n",
       "      <td>161.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.282312</td>\n",
       "      <td>42.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-1.263675</td>\n",
       "      <td>0.762930</td>\n",
       "      <td>175993.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gap_2_down_time__mean  gap_2_down_time__std  gap_2_down_time__min  \\\n",
       "0            1399.497065           6198.512184                  46.0   \n",
       "1            1433.109706           7173.079166                  53.0   \n",
       "2             850.318820           5817.295774                  67.0   \n",
       "\n",
       "   gap_2_down_time__max  gap_2_down_time__first  gap_2_down_time__last  \\\n",
       "0              162229.0                102045.0                19815.0   \n",
       "1              155377.0                  2083.0               115062.0   \n",
       "2              154879.0                 16894.0                 1602.0   \n",
       "\n",
       "   gap_2_down_time__sem  gap_2_down_time__q1  gap_2_down_time__median  \\\n",
       "0            122.628667                250.0                    344.0   \n",
       "1            144.858969                256.0                    344.0   \n",
       "2             90.476524                161.0                    196.0   \n",
       "\n",
       "   gap_2_down_time__q3  ...  gap_500_word_count__first  \\\n",
       "0                674.5  ...                       42.0   \n",
       "1                616.0  ...                       82.0   \n",
       "2                284.0  ...                       48.0   \n",
       "\n",
       "   gap_500_word_count__last  gap_500_word_count__sem  gap_500_word_count__q1  \\\n",
       "0                      46.0                 0.214883                    42.0   \n",
       "1                      36.0                 0.287947                    62.0   \n",
       "2                      62.0                 0.282312                    42.0   \n",
       "\n",
       "   gap_500_word_count__median  gap_500_word_count__q3  \\\n",
       "0                        54.0                    60.0   \n",
       "1                        65.0                    83.0   \n",
       "2                        52.0                    61.0   \n",
       "\n",
       "   gap_500_word_count__iqr  gap_500_word_count__skew  \\\n",
       "0                     18.0                 -0.246747   \n",
       "1                     21.0                  0.137884   \n",
       "2                     19.0                 -1.263675   \n",
       "\n",
       "   gap_500_word_count__Series.kurt  gap_500_word_count__sum  \n",
       "0                        -1.263949                 106495.0  \n",
       "1                        -0.621870                 135714.0  \n",
       "2                         0.762930                 175993.0  \n",
       "\n",
       "[3 rows x 336 columns]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaps = [2, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "def iqr(x):\n",
    "    return x.quantile(0.75) - x.quantile(.25)\n",
    "\n",
    "AGGREGATIONS = ['mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, iqr, 'skew', pd.Series.kurtosis, 'sum']\n",
    "\n",
    "metrics = ['down_time', 'cursor_position', 'word_count']\n",
    "\n",
    "def get_gaps_info(logs_df):\n",
    "    data = pd.DataFrame()\n",
    "    data['id'] = logs_df['id'].unique().tolist()\n",
    "    temp = logs_df.copy()\n",
    "    for metric in metrics:\n",
    "        for gap in gaps:\n",
    "            temp[f'gap_{gap}_{metric}'] = temp[metric] - temp.groupby('id')[metric].shift(gap)\n",
    "            for agg in AGGREGATIONS:\n",
    "                agg_name = str(agg)\n",
    "                if agg_name[0] == '<':\n",
    "                    agg_name = agg_name.split(' ')[1]\n",
    "                data[f'gap_{gap}_{metric}__{agg_name}'] = temp.groupby('id')[f'gap_{gap}_{metric}'].agg(agg).values\n",
    "    return data.drop('id', axis=1)\n",
    "gap_info = get_gaps_info(train_logs)\n",
    "gap_info = gap_info.fillna(0)\n",
    "gap_info.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.44476798525890426\n",
      "MSQE: validation set\n",
      "0.6864302451195533\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = gap_info, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern info -- train score: 0.792, valid score: 0.856\n",
      "n-gram tdidf -- train score: 0.405, valid score: 0.693\n",
      "iki info -- train score: 0.536, valid score: 0.714\n",
      "agg ingo -- train score: 0.459, valid score: 0.656\n",
      "other counts info -- train score: 0.568, valid score: 0.745\n",
      "gap info -- train score: 0.439, valid score: 0.671\n"
     ]
    }
   ],
   "source": [
    "feature_dfs = [\n",
    "    ('pattern info', pattern_info),\n",
    "    ('n-gram tdidf', train_ngram_tdidf),\n",
    "    ('iki info', iki_info),\n",
    "    ('agg ingo', agg_info),\n",
    "    ('other counts info', other_counts_info),\n",
    "    ('gap info', gap_info)\n",
    "]\n",
    "feature_importance_dfs = []\n",
    "for name, df in feature_dfs:\n",
    "    models = []\n",
    "    train_msqe_scores = []\n",
    "    valid_msqe_scores = []\n",
    "    for rs in [1, 2, 3, 4, 5]:\n",
    "        X, y = df, train_scores['score']\n",
    "        X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "        model = GradientBoostingRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict_train = model.predict(X_train)\n",
    "        y_predict_valid = model.predict(X_valid)\n",
    "        models.append(model)\n",
    "        train_msqe_scores.append(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "        valid_msqe_scores.append(mean_squared_error(y_valid, y_predict_valid, squared=False))\n",
    "    print(f'{name} -- train score: {round(np.mean(train_msqe_scores), 3)}, valid score: {round(np.mean(valid_msqe_scores), 3)}')\n",
    "    feature_importances_values = np.asarray([model.feature_importances_ for model in models]).mean(axis=0)\n",
    "    feature_importance_df = pd.DataFrame({'name': df.columns, 'importance': feature_importances_values})\n",
    "    feature_importance_dfs.append((name, feature_importance_df.sort_values('importance', ascending=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pattern info: 10\n",
    "n-gram tdidf: 37\n",
    "iki info: 27\n",
    "agg info: 43\n",
    "other counts info: 20\n",
    "gap info: 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2471, 174)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_numbers = [10, 37, 27, 43, 20, 37]\n",
    "train_feats = pd.DataFrame()\n",
    "for i in range(len(feature_numbers)):\n",
    "    cols = feature_importance_dfs[i][1]['name'][:feature_numbers[i]]\n",
    "    for col in cols:\n",
    "        train_feats[col] = feature_dfs[i][1][col]\n",
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "to_drop = [\n",
    "    'snt_q_count__sum',\n",
    "    'par_q_count__sum',\n",
    "    'par_len__sum',\n",
    "    'gap_200_word_count__sum',\n",
    "    'gap_100_word_count__sum',\n",
    "    'gap_50_word_count__sum',\n",
    "    'gap_2_word_count__sum',\n",
    "    'word__count',\n",
    "    'gap_20_word_count__sum',\n",
    "    'gap_10_word_count__sum',\n",
    "    'gap_5_word_count__sum',\n",
    "    '1000ms_long_run_count',\n",
    "    '300ms_long_run_count',\n",
    "    '500ms_run_count',\n",
    "    '1000ms_run_count',\n",
    "    '100ms_run_count',\n",
    "    'portion_ikis_under_300ms',\n",
    "    'portion_ikis_under_1000ms',\n",
    "    'portion_ikis_under_100ms',\n",
    "    'portion_ikis_under_2000ms',\n",
    "    'portion_ikis_under_10000ms',\n",
    "    'portion_ikis_under_60000ms',\n",
    "    'par_len__mean',\n",
    "    'snt_len__sum',\n",
    "    'gap_2_down_time__iqr',\n",
    "    'gap_2_down_time__q1',\n",
    "    'par_word_count__mean',\n",
    "    'par_q_count__q1',\n",
    "    'text_change_._count'\n",
    "\n",
    "]\n",
    "tf2 = train_feats.copy()\n",
    "tf2 = tf2.drop(to_drop, axis=1)\n",
    "data = pd.DataFrame()\n",
    "data['features'] = tf2.columns\n",
    "data['VIF'] = [variance_inflation_factor(tf2, i).round(1) for i in range(tf2.shape[1])]\n",
    "f_scores, p_scores = f_regression(tf2, train_scores['score'])\n",
    "data['Fscores'] = f_scores.round(0)\n",
    "data['Pscores'] = p_scores.round(5)\n",
    "to_drop = data.sort_values(by='Fscores', ascending=False)['features'][100:]\n",
    "tf3 = tf2.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.42343974696801734\n",
      "MSQE: validation set\n",
      "0.6270746487347747\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline prediction model\n",
    "\n",
    "X, y = tf3, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=1.0; total time=   3.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=50, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.7; total time=  14.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.7; total time=  14.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.7; total time=  14.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=0.7; total time=  14.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=1.0; total time=  20.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=1.0; total time=  18.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=1.0; total time=  18.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=1.0; total time=  19.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=3, n_estimators=500, subsample=1.0; total time=  18.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.5; total time=   5.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.7; total time=   7.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.7; total time=   7.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.5; total time=  22.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.5; total time=  22.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.7; total time=  29.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.7; total time=  29.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.7; total time=  29.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.7; total time=  29.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=0.7; total time=  29.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=1.0; total time=  44.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=1.0; total time=  48.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=1.0; total time=  42.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=1.0; total time=  48.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=7, n_estimators=500, subsample=1.0; total time=  51.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.7; total time=   1.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=0.7; total time=   1.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.5; total time=   3.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.5; total time=   6.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.5; total time=   6.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.7; total time=   9.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.8s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.6s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.4s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=1.0; total time=  11.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=100, subsample=1.0; total time=  11.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.5; total time=  29.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.5; total time=  26.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.5; total time=  26.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.5; total time=  25.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.7; total time=  36.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.7; total time=  39.9s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.7; total time=  38.7s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.7; total time=  40.5s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=0.7; total time=  47.3s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=1.0; total time=  57.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=1.0; total time=  56.1s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=1.0; total time=  52.2s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=1.0; total time=  53.0s\n",
      "[CV] END learning_rate=0.0001, max_depth=9, n_estimators=500, subsample=1.0; total time=  57.8s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.5s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.7s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.0s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.8s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=1.0; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=1.0; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=1.0; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=1.0; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=50, subsample=1.0; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.8s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.7s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.9s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.5s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.5s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.5s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.4s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.7s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.4s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.0s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.1s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.1s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.1s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=1.0; total time=  19.1s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=1.0; total time=  18.3s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=1.0; total time=  19.4s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=1.0; total time=  19.7s\n",
      "[CV] END learning_rate=0.001, max_depth=3, n_estimators=500, subsample=1.0; total time=  20.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.0s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.4s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.4s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.6s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.0s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.7; total time=   7.0s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.8s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.4s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.8s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.8s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.7s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.5; total time=  22.3s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.5; total time=  20.8s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.5; total time=  23.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.5; total time=  24.3s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.7; total time=  28.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.7; total time=  31.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.7; total time=  33.3s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.7; total time=  29.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=0.7; total time=  28.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=1.0; total time=  40.7s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=1.0; total time=  41.1s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=1.0; total time=  40.7s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=1.0; total time=  40.9s\n",
      "[CV] END learning_rate=0.001, max_depth=7, n_estimators=500, subsample=1.0; total time=  41.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.9s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.6s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.6s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.6s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.6s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.7; total time=   5.6s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.7; total time=   8.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.7; total time=   8.8s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.7; total time=   6.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=0.7; total time=   5.3s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.3s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.5; total time=   6.1s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.5; total time=   6.5s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.5; total time=   7.1s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.9s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.8s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.7; total time=  12.4s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.4s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=1.0; total time=  11.1s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.8s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=1.0; total time=  10.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=100, subsample=1.0; total time=  11.4s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.5; total time=  33.4s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.5; total time=  30.6s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.3s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.5; total time=  32.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.5; total time=  30.4s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.7; total time=  44.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.7; total time=  46.7s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.7; total time=  42.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.7; total time=  43.4s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=0.7; total time=  45.9s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=1.0; total time= 1.1min\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=1.0; total time= 1.2min\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=1.0; total time=  57.0s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=1.0; total time=  53.6s\n",
      "[CV] END learning_rate=0.001, max_depth=9, n_estimators=500, subsample=1.0; total time=  56.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.5; total time=  11.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.5; total time=  11.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.5; total time=  11.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.7; total time=  14.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.7; total time=  14.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=0.7; total time=  15.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=1.0; total time=  43.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=1.0; total time=  23.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=1.0; total time=  24.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=1.0; total time=  33.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=500, subsample=1.0; total time=  32.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=  10.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   8.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   9.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.5; total time=  26.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.5; total time=  22.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.7; total time=  34.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.7; total time=  37.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.7; total time=  31.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.7; total time=  34.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=0.7; total time=  36.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=1.0; total time=  50.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=1.0; total time=  48.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=1.0; total time=  54.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=1.0; total time=  57.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=500, subsample=1.0; total time= 1.1min\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.5; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.5; total time=   1.7s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.5; total time=   1.7s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.5; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.5; total time=   1.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.7; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.7; total time=   2.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.7; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=0.7; total time=   2.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=1.0; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=1.0; total time=   4.4s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=1.0; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=10, subsample=1.0; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.5; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.5; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.5; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.5; total time=   3.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.5; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.4s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=1.0; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=1.0; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=50, subsample=1.0; total time=   6.1s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.5; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.5; total time=   6.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.5; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.3s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=  11.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=  24.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=  14.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=  11.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=100, subsample=1.0; total time=  11.4s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.5; total time=  29.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.7s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.5; total time=  29.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.3s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.7; total time=  45.0s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.7; total time=  48.4s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.7; total time=  54.3s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.7; total time=  48.4s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=0.7; total time=  40.2s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=1.0; total time=  56.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=1.0; total time=  56.7s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=1.0; total time=  58.1s\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=1.0; total time= 1.0min\n",
      "[CV] END learning_rate=0.01, max_depth=9, n_estimators=500, subsample=1.0; total time= 1.0min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0; total time=   1.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.5; total time=  11.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=  18.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=  17.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=  16.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=1.0; total time=  21.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=1.0; total time=  22.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=1.0; total time=  22.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=1.0; total time=  20.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=1.0; total time=  22.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.3s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.3s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.5s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.8s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.7s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.5s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.7; total time=   7.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.7; total time=   8.7s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.7; total time=   7.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=  12.4s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   7.7s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   7.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   7.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   7.5s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.5; total time=  18.8s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.5; total time=  19.4s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.5; total time=  18.7s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.5; total time=  18.5s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.5; total time=  19.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.7; total time=  26.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.7; total time=  26.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.7; total time=  26.2s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.7; total time=  26.8s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.7; total time=  25.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=1.0; total time=  37.6s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=1.0; total time=  37.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=1.0; total time=  36.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=1.0; total time=  51.9s\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=500, subsample=1.0; total time=  37.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.0s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.4s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.2s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.2s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.2s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=1.0; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=1.0; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=1.0; total time=   4.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=50, subsample=1.0; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.4s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.4s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.8s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.7s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.7; total time=   6.4s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.7s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=   9.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=100, subsample=1.0; total time=  14.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.5; total time=  34.8s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.5; total time=  23.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.5; total time=  23.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.5; total time=  23.7s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.5; total time=  23.2s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.7; total time=  31.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.7; total time=  40.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.7; total time=  37.8s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.7; total time=  32.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=0.7; total time=  31.3s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=1.0; total time=  46.6s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=1.0; total time=  46.8s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=1.0; total time=  47.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=1.0; total time=  46.9s\n",
      "[CV] END learning_rate=0.1, max_depth=9, n_estimators=500, subsample=1.0; total time=  58.8s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=0.7; total time=   0.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=10, subsample=1.0; total time=   0.5s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.6s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.5; total time=   1.0s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.5s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.6s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.5s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=50, subsample=1.0; total time=   2.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.7; total time=   4.0s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.7; total time=   2.9s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=0.7; total time=   3.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.5s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.1s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.8s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=100, subsample=1.0; total time=   3.7s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.5; total time=  11.3s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.1s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.7s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.5; total time=  10.1s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.5; total time=   9.9s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.7; total time=  15.6s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.7; total time=  17.8s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=0.7; total time=  13.6s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=1.0; total time=  19.0s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=1.0; total time=  19.4s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=1.0; total time=  18.9s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=1.0; total time=  18.8s\n",
      "[CV] END learning_rate=1.0, max_depth=3, n_estimators=500, subsample=1.0; total time=  19.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.5; total time=   0.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=0.7; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=10, subsample=1.0; total time=   0.9s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.5; total time=   2.2s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.0s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=0.7; total time=   3.2s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=50, subsample=1.0; total time=   4.3s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.3s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.0s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.0s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=0.7; total time=   6.0s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.9s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=100, subsample=1.0; total time=   8.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.8s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.8s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.5; total time=  21.6s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.5; total time=  22.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.5; total time=  22.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.7; total time=  19.6s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.7; total time=  18.7s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.7; total time=  18.4s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.7; total time=  19.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=0.7; total time=  18.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=1.0; total time=   9.8s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=1.0; total time=   9.5s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=1.0; total time=   8.9s\n",
      "[CV] END learning_rate=1.0, max_depth=7, n_estimators=500, subsample=1.0; total time=   9.0s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.5; total time=   0.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=10, subsample=1.0; total time=   1.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.9s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.5; total time=   3.0s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.5; total time=   2.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.5; total time=   3.0s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.9s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.0s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.7; total time=   4.2s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.9s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=0.7; total time=   3.9s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=50, subsample=1.0; total time=   5.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.9s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.0s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.7; total time=   8.0s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.9s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=0.7; total time=   7.8s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=100, subsample=1.0; total time=   6.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.5; total time=  29.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.3s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.5; total time=  28.4s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.7; total time=  18.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.7; total time=  17.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.7; total time=  16.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.7; total time=  16.6s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=0.7; total time=  17.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=1.0; total time=   7.1s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=1.0; total time=   6.5s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=1.0; total time=   6.3s\n",
      "[CV] END learning_rate=1.0, max_depth=9, n_estimators=500, subsample=1.0; total time=   6.6s\n",
      "Best Score: 0.6169252572217653 Best Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = tf3.rename(columns={x:y for x,y in zip(tf3.columns,range(0,len(tf3.columns)))}), train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "params = {\n",
    "    'application': 'binary', # for binary classification\n",
    "    'boosting': 'gbdt', # traditional gradient boosting decision tree\n",
    "    'num_iterations': 100, \n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 62,\n",
    "    'max_depth': -1, # <0 means no limit\n",
    "    'max_bin': 510, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "    'lambda_l1': 5, # L1 regularization\n",
    "    'lambda_l2': 10, # L2 regularization\n",
    "    'metric' : 'binary_error',\n",
    "    'subsample_for_bin': 200, # number of samples for constructing bins\n",
    "    'subsample': 1, # subsample ratio of the training instance\n",
    "    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n",
    "    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "    'min_child_samples': 5# minimum number of data needed in a leaf\n",
    "}\n",
    "model = GradientBoostingRegressor()\n",
    "grid = {}\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "grid['subsample'] = [0.5, 0.7, 1.0]\n",
    "grid['max_depth'] = [3, 7, 9]\n",
    "gs = GridSearchCV(estimator=model, \n",
    "                           param_grid=grid,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           verbose=2,\n",
    "                           cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "results = gs.cv_results_\n",
    "print('Best Score:', -gs.best_score_, 'Best Params:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSQE: training set\n",
      "0.4130375106429742\n",
      "MSQE: validation set\n",
      "0.6108567957677844\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline prediction model\n",
    "\n",
    "X, y = tf3.rename(columns={x:y for x,y in zip(tf3.columns,range(0,len(tf3.columns)))}), train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': .01,\n",
    "    'max_depth' : 3,\n",
    "    'subsample' : .5}\n",
    "model = GradientBoostingRegressor(**params)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = model.predict(X_train)\n",
    "y_predict_valid = model.predict(X_valid)\n",
    "print('MSQE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MSQE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
