{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataFrames\n",
    "train_logs = pd.read_csv('input/train_logs.csv')\n",
    "test_logs = pd.read_csv('input/test_logs.csv')\n",
    "train_scores = pd.read_csv('input/train_scores.csv')\n",
    "train_essays = pd.read_csv('output/train_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish some variables and functions for testing and evaluating\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rs = 0\n",
    "def round_scores(scores):\n",
    "    scores = ((scores * 2) // 1) * .5\n",
    "    scores = np.where(scores < 0, 0.0, scores)\n",
    "    scores = np.where(scores > 6, 6.0, scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract several different kinds of features and add them to separate dataframes\n",
    "1. Word frequency (using TF-IDF scores) from the \"words\" of the reconstructed essays\n",
    "2. Aggregations of events, text changes, &c along with event index gaps\n",
    "3. Gap info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with \"word\" analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_tdidf(essay_df):\n",
    "    # Get counts of all 1, 2, and 3-grams\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "    X_tokenizer_train = count_vectorizer.fit_transform(essay_df['essay']).todense()\n",
    "\n",
    "    # Create a matrix of 1,0 values to keep track of which grams appear in which essays at least once\n",
    "    X_tokenizer_reduced = X_tokenizer_train.copy()\n",
    "    for i in reversed(range(X_tokenizer_reduced.shape[1])):\n",
    "        X_tokenizer_reduced[:,i] = np.where(X_tokenizer_reduced[:,i] > 0, 1, 0)\n",
    "\n",
    "    # Create tdidf DataFrame\n",
    "    col_names = [f'word_group_{i}' for i in range(X_tokenizer_train.shape[1])]\n",
    "    X_tokenizer_final = pd.DataFrame(X_tokenizer_train.copy(), columns=col_names)\n",
    "    idfs = []\n",
    "    for i in range(X_tokenizer_final.shape[1]):\n",
    "        idfs.append(np.log(X_tokenizer_final.shape[0] / np.sum(X_tokenizer_reduced[:,i])))\n",
    "    def compute_tfidf(row):\n",
    "        sum = np.sum([row[col] for col in col_names])\n",
    "        for i in range(X_tokenizer_train.shape[1]):\n",
    "            row[f'word_group_{i}'] = row[f'word_group_{i}'] * idfs[i] / sum\n",
    "        return row\n",
    "    X_tokenizer_final = X_tokenizer_final.apply(compute_tfidf, axis = 1)\n",
    "    return X_tokenizer_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_group_0</th>\n",
       "      <th>word_group_1</th>\n",
       "      <th>word_group_2</th>\n",
       "      <th>word_group_3</th>\n",
       "      <th>word_group_4</th>\n",
       "      <th>word_group_5</th>\n",
       "      <th>word_group_6</th>\n",
       "      <th>word_group_7</th>\n",
       "      <th>word_group_8</th>\n",
       "      <th>word_group_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_group_3385</th>\n",
       "      <th>word_group_3386</th>\n",
       "      <th>word_group_3387</th>\n",
       "      <th>word_group_3388</th>\n",
       "      <th>word_group_3389</th>\n",
       "      <th>word_group_3390</th>\n",
       "      <th>word_group_3391</th>\n",
       "      <th>word_group_3392</th>\n",
       "      <th>word_group_3393</th>\n",
       "      <th>word_group_3394</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 3395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_group_0  word_group_1  word_group_2  word_group_3  word_group_4  \\\n",
       "0           0.0           0.0           0.0           0.0      0.000152   \n",
       "1           0.0           0.0           0.0           0.0      0.000141   \n",
       "2           0.0           0.0           0.0           0.0      0.000084   \n",
       "\n",
       "   word_group_5  word_group_6  word_group_7  word_group_8  word_group_9  ...  \\\n",
       "0      0.000928      0.000472      0.001125      0.000711      0.000770  ...   \n",
       "1      0.000753      0.000767      0.000609      0.000000      0.000000  ...   \n",
       "2      0.000512      0.000260      0.000207      0.000392      0.000849  ...   \n",
       "\n",
       "   word_group_3385  word_group_3386  word_group_3387  word_group_3388  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_group_3389  word_group_3390  word_group_3391  word_group_3392  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_group_3393  word_group_3394  \n",
       "0              0.0              0.0  \n",
       "1              0.0              0.0  \n",
       "2              0.0              0.0  \n",
       "\n",
       "[3 rows x 3395 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_tdidf = get_tdidf(train_essays)\n",
    "train_ngram_tdidf.to_csv('output/train_ngram_tdidf.csv')\n",
    "train_ngram_tdidf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: training set\n",
      "0.4933771910329651\n",
      "MAE: validation set\n",
      "0.7763586758208265\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = train_ngram_tdidf, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = round_scores(model.predict(X_train))\n",
    "y_predict_valid = round_scores(model.predict(X_valid))\n",
    "print('MAE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MAE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate gap info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longest_iki</th>\n",
       "      <th>median_iki</th>\n",
       "      <th>mean_iki</th>\n",
       "      <th>initial_pause</th>\n",
       "      <th>start_time</th>\n",
       "      <th>100ms_run_count</th>\n",
       "      <th>100ms_long_run_count</th>\n",
       "      <th>100ms_max_run</th>\n",
       "      <th>portion_ikis_under_100ms</th>\n",
       "      <th>300ms_run_count</th>\n",
       "      <th>...</th>\n",
       "      <th>10000ms_max_run</th>\n",
       "      <th>portion_ikis_under_10000ms</th>\n",
       "      <th>30000ms_run_count</th>\n",
       "      <th>30000ms_long_run_count</th>\n",
       "      <th>30000ms_max_run</th>\n",
       "      <th>portion_ikis_under_30000ms</th>\n",
       "      <th>60000ms_run_count</th>\n",
       "      <th>60000ms_long_run_count</th>\n",
       "      <th>60000ms_max_run</th>\n",
       "      <th>portion_ikis_under_60000ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.136</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.610944</td>\n",
       "      <td>101.609</td>\n",
       "      <td>4526</td>\n",
       "      <td>1618</td>\n",
       "      <td>245.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.686742</td>\n",
       "      <td>2065</td>\n",
       "      <td>...</td>\n",
       "      <td>389</td>\n",
       "      <td>0.991396</td>\n",
       "      <td>2549</td>\n",
       "      <td>2549</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.998045</td>\n",
       "      <td>2552</td>\n",
       "      <td>2552</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.999218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145.899</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.620108</td>\n",
       "      <td>1.696</td>\n",
       "      <td>30623</td>\n",
       "      <td>1603</td>\n",
       "      <td>640.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.689487</td>\n",
       "      <td>2047</td>\n",
       "      <td>...</td>\n",
       "      <td>770</td>\n",
       "      <td>0.988183</td>\n",
       "      <td>2443</td>\n",
       "      <td>2431</td>\n",
       "      <td>1459</td>\n",
       "      <td>0.996740</td>\n",
       "      <td>2449</td>\n",
       "      <td>2449</td>\n",
       "      <td>2321</td>\n",
       "      <td>0.998778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153.886</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.360506</td>\n",
       "      <td>16.736</td>\n",
       "      <td>4441</td>\n",
       "      <td>3333</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.821809</td>\n",
       "      <td>3794</td>\n",
       "      <td>...</td>\n",
       "      <td>681</td>\n",
       "      <td>0.995164</td>\n",
       "      <td>4123</td>\n",
       "      <td>4123</td>\n",
       "      <td>766</td>\n",
       "      <td>0.997340</td>\n",
       "      <td>4130</td>\n",
       "      <td>4130</td>\n",
       "      <td>2528</td>\n",
       "      <td>0.999033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   longest_iki  median_iki  mean_iki  initial_pause  start_time  \\\n",
       "0      154.136       0.062  0.610944        101.609        4526   \n",
       "1      145.899       0.061  0.620108          1.696       30623   \n",
       "2      153.886       0.040  0.360506         16.736        4441   \n",
       "\n",
       "   100ms_run_count  100ms_long_run_count  100ms_max_run  \\\n",
       "0             1618                 245.0             24   \n",
       "1             1603                 640.0             61   \n",
       "2             3333                2300.0             39   \n",
       "\n",
       "   portion_ikis_under_100ms  300ms_run_count  ...  10000ms_max_run  \\\n",
       "0                  0.686742             2065  ...              389   \n",
       "1                  0.689487             2047  ...              770   \n",
       "2                  0.821809             3794  ...              681   \n",
       "\n",
       "   portion_ikis_under_10000ms  30000ms_run_count  30000ms_long_run_count  \\\n",
       "0                    0.991396               2549                    2549   \n",
       "1                    0.988183               2443                    2431   \n",
       "2                    0.995164               4123                    4123   \n",
       "\n",
       "   30000ms_max_run  portion_ikis_under_30000ms  60000ms_run_count  \\\n",
       "0             1338                    0.998045               2552   \n",
       "1             1459                    0.996740               2449   \n",
       "2              766                    0.997340               4130   \n",
       "\n",
       "   60000ms_long_run_count  60000ms_max_run  portion_ikis_under_60000ms  \n",
       "0                    2552             2500                    0.999218  \n",
       "1                    2449             2321                    0.998778  \n",
       "2                    4130             2528                    0.999033  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_iki_info(logs_df):\n",
    "    logs_df['prev_up_time'] = logs_df.groupby('id')['up_time'].shift(1).fillna(logs_df['down_time'])\n",
    "    logs_df['inter_keystroke_intvl'] = (abs(logs_df['down_time'] - logs_df['prev_up_time'])) / 1000\n",
    "    group = logs_df.groupby('id')['inter_keystroke_intvl']\n",
    "    total_ikis = group.count()\n",
    "   \n",
    "    data =  pd.DataFrame({\n",
    "        'id': logs_df['id'].unique(),\n",
    "        'longest_iki': group.max(),\n",
    "        'median_iki': group.median(),\n",
    "        'mean_iki': group.mean(),\n",
    "        'initial_pause': group.apply(lambda ikis: max(ikis.values[:10])),\n",
    "        'start_time': logs_df.groupby('id')['down_time'].first()\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    ms_thresholds = [100, 300, 500, 1000, 2000, 10000, 30000, 60000]\n",
    "    for ms in ms_thresholds:\n",
    "        def get_runs(ikis):\n",
    "            runs = []\n",
    "            curr_run = 0\n",
    "            for i, val in enumerate(ikis.values[1:]):\n",
    "                if val > ms / 1000 or i == len(ikis.values) - 2:\n",
    "                    if curr_run >= 2:\n",
    "                        runs.append(curr_run)\n",
    "                    curr_run = 0\n",
    "                else:\n",
    "                    curr_run += 1\n",
    "            return runs\n",
    "        runs_series = group.apply(get_runs)\n",
    "        # count total number of ikis in runs (not number of runs)\n",
    "        data[f'{ms}ms_run_count'] = runs_series.apply(lambda runs: np.sum(runs)).values\n",
    "        data[f'{ms}ms_long_run_count'] = runs_series.apply(lambda runs: np.sum([n for n in runs if n >= 10])).values\n",
    "        # count longest run\n",
    "        data[f'{ms}ms_max_run'] = runs_series.apply(lambda runs: max(runs)).values\n",
    "        # count portion of ikis under threshold\n",
    "        num_ikis = group.apply(lambda x: len([n for n in x.values if n < ms / 1000])).values\n",
    "        data[f'portion_ikis_under_{ms}ms'] = num_ikis / total_ikis.values\n",
    "    return data.drop('id', axis=1)\n",
    "iki_info = get_iki_info(train_logs[['id', 'up_time', 'down_time']])\n",
    "iki_info.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: training set\n",
      "0.6096114999216363\n",
      "MAE: validation set\n",
      "0.7899290184890109\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline model prediction\n",
    "\n",
    "X, y = iki_info, train_scores['score']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=rs)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.25, random_state=rs)\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train = round_scores(model.predict(X_train))\n",
    "y_predict_valid = round_scores(model.predict(X_valid))\n",
    "print('MAE: training set')\n",
    "print(mean_squared_error(y_train, y_predict_train, squared=False))\n",
    "print('MAE: validation set')\n",
    "print(mean_squared_error(y_valid, y_predict_valid, squared=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
